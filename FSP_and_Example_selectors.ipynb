{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\veace\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\deeplake\\util\\check_latest_version.py:32: UserWarning: A newer version of deeplake (3.8.1) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I be lovin' the art of code plunderin'\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts.chat import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
    "\n",
    "template = \"You are a helpful assistant that translates english to pirate.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "example_human = HumanMessagePromptTemplate.from_template(\"Hi\")\n",
    "example_ai = AIMessagePromptTemplate.from_template('Argh me mateys')\n",
    "human_template = \"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, example_human, example_ai, human_message_prompt])\n",
    "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
    "chain.run(\"I love programming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A lifetime supply of chocolate and a good sense of humor!'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate, FewShotPromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\"query\": \"What's the weather like?\",\n",
    "     \"answer\": \"It's raining cats and dogs, better bring and umbrella!\"},\n",
    "    {\"query\": \"How old are you?\",\n",
    "     \"answer\": \"Age is just a number, but I'm timeless.\"}\n",
    "]\n",
    "\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "Answer: {answer}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(input_variables=['query', 'answer'], template=example_template)\n",
    "\n",
    "prefix = \"\"\"The following are excerpts from conversations with an AI\n",
    "assistant. The assistant is known for its humor and wit, providing\n",
    "entertaining and amusing responses to users' questions. Here are some\n",
    "examples:\n",
    "\"\"\"\n",
    "\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \n",
    "\"\"\"\n",
    "\n",
    "few_shot_prompt_template = FewShotPromptTemplate(examples=examples,\n",
    "                                                 example_prompt=example_prompt,\n",
    "                                                 prefix=prefix,\n",
    "                                                 suffix=suffix,\n",
    "                                                 input_variables=['query'],\n",
    "                                                 example_separator=\"\\n\\n\")\n",
    "\n",
    "chain = LLMChain(llm=chat, prompt=few_shot_prompt_template)\n",
    "chain.run(\"What's the secret to happiness?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'small bad'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\"word\": \"happy\", \"antonym\": \"sad\"},\n",
    "    {\"word\": \"tall\", \"antonym\": \"short\"},\n",
    "    {\"word\": \"energetic\", \"antonym\": \"lethargic\"},\n",
    "    {\"word\": \"sunny\", \"antonym\": \"gloomy\"},\n",
    "    {\"word\": \"windy\", \"antonym\": \"calm\"},\n",
    "]\n",
    "\n",
    "example_template = \"\"\"\n",
    "Word: {word}\n",
    "Antonym: {antonym}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(input_variables=['word', 'antonym'], template=example_template)\n",
    "\n",
    "example_selector = LengthBasedExampleSelector(examples=examples, example_prompt=example_prompt, max_length=25)\n",
    "\n",
    "dynamic_prompt = FewShotPromptTemplate(example_selector=example_selector,\n",
    "                                       example_prompt=example_prompt,\n",
    "                                       prefix=\"Give the antonym of every input\",\n",
    "                                       suffix=\"Word: {input}\\nAntonym:\",\n",
    "                                       input_variables=['input'],\n",
    "                                       example_separator=\"\\n\\n\")\n",
    "\n",
    "chain = LLMChain(llm=chat, prompt=dynamic_prompt)\n",
    "chain.run(\"big good\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic similarity example selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in hub://veaceslavcalestru/langchain_course_fewshot_selector already exists, loading from the storage\n",
      "Deep Lake Dataset in ./deeplake/ already exists, loading from the storage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='./deeplake/', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype      shape      dtype  compression\n",
      "  -------    -------    -------    -------  ------- \n",
      " embedding  embedding  (10, 1536)  float32   None   \n",
      "    id        text      (10, 1)      str     None   \n",
      " metadata     json      (10, 1)      str     None   \n",
      "   text       text      (10, 1)      str     None   \n",
      "Convert the temperature from Celsius to Fahrenheit\n",
      "\n",
      "Input: 10°C\n",
      "Output: 50°F\n",
      "\n",
      "Input: 10°C\n",
      "Output:\n",
      "Convert the temperature from Celsius to Fahrenheit\n",
      "\n",
      "Input: 30°C\n",
      "Output: 86°F\n",
      "\n",
      "Input: 30°C\n",
      "Output:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='./deeplake/', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype      shape      dtype  compression\n",
      "  -------    -------    -------    -------  ------- \n",
      " embedding  embedding  (11, 1536)  float32   None   \n",
      "    id        text      (11, 1)      str     None   \n",
      " metadata     json      (11, 1)      str     None   \n",
      "   text       text      (11, 1)      str     None   \n",
      "Convert the temperature from Celsius to Fahrenheit\n",
      "\n",
      "Input: 40°C\n",
      "Output: 104°F\n",
      "\n",
      "Input: 40°C\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "example_prompt = PromptTemplate(input_variables=['input', 'output'],\n",
    "                                template=\"Input: {input}\\nOutput: {output}\")\n",
    "\n",
    "examples = [\n",
    "    {\"input\": \"0°C\", \"output\": \"32°F\"},\n",
    "    {\"input\": \"10°C\", \"output\": \"50°F\"},\n",
    "    {\"input\": \"20°C\", \"output\": \"68°F\"},\n",
    "    {\"input\": \"30°C\", \"output\": \"86°F\"},\n",
    "    {\"input\": \"40°C\", \"output\": \"104°F\"},\n",
    "]\n",
    "\n",
    "my_activeloop_org_id = 'veaceslavcalestru'\n",
    "my_activeloop_dataset_name = \"langchain_course_fewshot_selector\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "db = DeepLake(dataset_path=dataset_path)\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(examples, embeddings, db, k=1)\n",
    "\n",
    "similar_prompt = FewShotPromptTemplate(example_selector=example_selector,\n",
    "                                       example_prompt=example_prompt,\n",
    "                                       prefix=\"Convert the temperature from Celsius to Fahrenheit\",\n",
    "                                       suffix=\"Input: {temperature}\\nOutput:\",\n",
    "                                       input_variables=['temperature'])\n",
    "\n",
    "print(similar_prompt.format(temperature=\"10°C\"))\n",
    "print(similar_prompt.format(temperature=\"30°C\"))\n",
    "\n",
    "similar_prompt.example_selector.add_example({\"input\": \"50°C\", \"output\": \"122°F\"})\n",
    "print(similar_prompt.format(temperature=\"40°C\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
