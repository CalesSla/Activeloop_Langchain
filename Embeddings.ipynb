{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\veace\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\deeplake\\util\\check_latest_version.py:32: UserWarning: A newer version of deeplake (3.8.2) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar document to the query\n",
      "'A cat is sitting on a mat.':\n",
      "The cat is on the mat.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "documents = [\n",
    "    \"The cat is on the mat.\",\n",
    "    \"There is a cat on the mat.\",\n",
    "    \"The dog is in the yard.\",\n",
    "    \"There is a dog in the yard.\",\n",
    "]\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "document_embeddings = embeddings.embed_documents(documents)\n",
    "\n",
    "query = \"A cat is sitting on a mat.\"\n",
    "query_embedding = embeddings.embed_query(query)\n",
    "\n",
    "similarity_scores = cosine_similarity([query_embedding], document_embeddings)\n",
    "\n",
    "most_similar_index = np.argmax(similarity_scores)\n",
    "most_similar_document = documents[most_similar_index]\n",
    "\n",
    "print(f\"Most similar document to the query\\n'{query}':\")\n",
    "print(most_similar_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\veace\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)a8e1d/.gitattributes: 100%|██████████| 1.18k/1.18k [00:00<?, ?B/s]\n",
      "Downloading (…)_Pooling/config.json: 100%|██████████| 190/190 [00:00<00:00, 191kB/s]\n",
      "Downloading (…)b20bca8e1d/README.md: 100%|██████████| 10.6k/10.6k [00:00<00:00, 11.9MB/s]\n",
      "Downloading (…)0bca8e1d/config.json: 100%|██████████| 571/571 [00:00<?, ?B/s] \n",
      "Downloading (…)ce_transformers.json: 100%|██████████| 116/116 [00:00<?, ?B/s] \n",
      "Downloading (…)e1d/data_config.json: 100%|██████████| 39.3k/39.3k [00:00<00:00, 332kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 438M/438M [02:14<00:00, 3.25MB/s] \n",
      "Downloading (…)nce_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<?, ?B/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 239/239 [00:00<00:00, 266kB/s]\n",
      "Downloading (…)a8e1d/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 1.14MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 363/363 [00:00<00:00, 400kB/s]\n",
      "Downloading (…)8e1d/train_script.py: 100%|██████████| 13.1k/13.1k [00:00<00:00, 12.9MB/s]\n",
      "Downloading (…)b20bca8e1d/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.57MB/s]\n",
      "Downloading (…)bca8e1d/modules.json: 100%|██████████| 349/349 [00:00<00:00, 375kB/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "hf = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
    "\n",
    "documents = [\"Document 1\", \"Document 2\", \"Document 3\"]\n",
    "\n",
    "doc_embeddings = hf.embed_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cohere embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextL Hello from Cohere!\n",
      "Embedding: [0.23449707, 0.50146484, -0.048797607, 0.13989258, -0.18005371]\n",
      "TextL مرحبًا من كوهير!\n",
      "Embedding: [0.25341797, 0.30029297, 0.010414124, 0.12585449, -0.18237305]\n",
      "TextL Hallo von Cohere!\n",
      "Embedding: [0.10290527, 0.2836914, -0.049560547, 0.23706055, -0.07165527]\n",
      "TextL Bonjour de Cohere!\n",
      "Embedding: [0.15161133, 0.28222656, -0.057128906, 0.117370605, -0.04421997]\n",
      "TextL ¡Hola desde Cohere!\n",
      "Embedding: [0.2512207, 0.43139648, -0.086120605, 0.24658203, -0.11669922]\n",
      "TextL Olá do Cohere!\n",
      "Embedding: [0.18664551, 0.39086914, -0.046051025, 0.14562988, -0.11242676]\n",
      "TextL Ciao da Cohere!\n",
      "Embedding: [0.115722656, 0.43310547, -0.026016235, 0.14526367, 0.07104492]\n",
      "TextL 您好，来自 Cohere！\n",
      "Embedding: [0.24597168, 0.3088379, -0.11212158, 0.26611328, -0.0513916]\n",
      "TextL कोहेरे से नमस्ते!\n",
      "Embedding: [0.1928711, 0.6352539, 0.03225708, 0.11767578, -0.2607422]\n"
     ]
    }
   ],
   "source": [
    "import cohere\n",
    "from langchain.embeddings import CohereEmbeddings\n",
    "\n",
    "cohere = CohereEmbeddings(model=\"embed-multilingual-v2.0\", cohere_api_key=\"Ash75qZAwm2vcLsSyKTKJzxPXSiwiuuSYvjlVHXw\")\n",
    "\n",
    "texts = [\n",
    "    \"Hello from Cohere!\", \n",
    "    \"مرحبًا من كوهير!\", \n",
    "    \"Hallo von Cohere!\",  \n",
    "    \"Bonjour de Cohere!\", \n",
    "    \"¡Hola desde Cohere!\", \n",
    "    \"Olá do Cohere!\",  \n",
    "    \"Ciao da Cohere!\", \n",
    "    \"您好，来自 Cohere！\", \n",
    "    \"कोहेरे से नमस्ते!\"\n",
    "]\n",
    "\n",
    "document_embeddings = cohere.embed_documents(texts)\n",
    "\n",
    "for text, embedding in zip(texts, document_embeddings):\n",
    "    print(f\"TextL {text}\")\n",
    "    print(f\"Embedding: {embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeepLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Deep Lake dataset has been successfully created!\n",
      "The dataset is private so make sure you are logged in!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://veaceslavcalestru/langchain_course_embeddings', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype      shape     dtype  compression\n",
      "  -------    -------    -------   -------  ------- \n",
      " embedding  embedding  (4, 1536)  float32   None   \n",
      "    id        text      (4, 1)      str     None   \n",
      " metadata     json      (4, 1)      str     None   \n",
      "   text       text      (4, 1)      str     None   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['044bbcc8-6e70-11ee-b60b-cc4740c98b6b',\n",
       " '044bbcc9-6e70-11ee-98a7-cc4740c98b6b',\n",
       " '044bbcca-6e70-11ee-b082-cc4740c98b6b',\n",
       " '044bbccb-6e70-11ee-b8f9-cc4740c98b6b']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "texts = [\n",
    "    \"Napoleon Bonaparte was born in 15 August 1769\",\n",
    "    \"Louis XIV was born in 5 September 1638\",\n",
    "    \"Lady Gaga was born in 28 March 1986\",\n",
    "    \"Michael Jeffrey Jordan was born in 17 February 1963\"\n",
    "]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.create_documents(texts)\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model = \"text-embedding-ada-002\")\n",
    "\n",
    "my_activeloop_org_id = \"veaceslavcalestru\"\n",
    "my_activeloop_dataset_name = \"langchain_course_embeddings\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
    "\n",
    "db.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Michael Jordan was born on 17 February 1963.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = db.as_retriever()\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "qa_chain = RetrievalQA.from_llm(model, retriever=retriever)\n",
    "\n",
    "qa_chain.run(\"When was Michael Jordan born?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
