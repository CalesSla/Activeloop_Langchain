{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Rainbow Socks Co.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name = 'text-davinci-003', temperature=0)\n",
    "\n",
    "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
    "\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tracking token usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 44\n",
      "\tPrompt Tokens: 4\n",
      "\tCompletion Tokens: 40\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.00088\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", n=2, best_of=2)\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    result = llm('Tell me a joke')\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few shot learning\n",
    "Learn and generalize from limited examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'42, according to \"The Hitchhiker\\'s Guide to the Galaxy\". But if you\\'re looking for a more philosophical answer, I\\'m afraid you\\'ll have to ask a philosopher. I\\'m just an AI!'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain import FewShotPromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"What's the weather like?\",\n",
    "        \"answer\": \"It's raining cats and dogs, better bring an umbrella!\"\n",
    "    }, {\n",
    "        \"query\": \"How old are you?\",\n",
    "        \"answer\": \"Age is just a number, but I'm timeless.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(input_variables=['query', 'answer'], template=example_template)\n",
    "\n",
    "prefix = \"\"\"\n",
    "The following are excerpts from conversations with an AI\n",
    "assistant. The assistant is known for its humor and wit, providing\n",
    "entertaining and amusing responses to users' questions. Here are some examples:\n",
    "\"\"\"\n",
    "\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \n",
    "\"\"\"\n",
    "\n",
    "few_shot_prompt_tempalte = FewShotPromptTemplate(examples=examples,\n",
    "                                                 example_prompt=example_prompt,\n",
    "                                                 prefix=prefix,\n",
    "                                                 suffix=suffix,\n",
    "                                                 input_variables=['query'],\n",
    "                                                 example_separator=\"\\n\\n\")\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import LLMChain\n",
    "\n",
    "chat = ChatOpenAI(model_name='gpt-4', temperature=0.0)\n",
    "\n",
    "chain = LLMChain(llm=chat, prompt=few_shot_prompt_tempalte)\n",
    "chain.run(\"What's the meaning of life?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples with easy prompts:\n",
    "* Text summarization\n",
    "* Text translation\n",
    "* Question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\veace\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\veace\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '0.19.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paris\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=['question'])\n",
    "\n",
    "question = \"What is the capital of France?\"\n",
    "\n",
    "from langchain import HuggingFaceHub, LLMChain\n",
    "\n",
    "hub_llm = HuggingFaceHub(repo_id = 'google/flan-t5-large', model_kwargs={'temperature': 0})\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=hub_llm)\n",
    "\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asking multiple questions\n",
    "Iteratoing over questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the capital city of France?\n",
      "Answer: paris\n",
      "\n",
      "Question: What is the largest mammal on Earth?\n",
      "Answer: giraffe\n",
      "\n",
      "Question: Which gas is most abundant in Earth's atmosphere?\n",
      "Answer: nitrogen\n",
      "\n",
      "Question: What color is a ripe banana?\n",
      "Answer: yellow\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qa = [\n",
    "    {'question': \"What is the capital city of France?\"},\n",
    "    {'question': \"What is the largest mammal on Earth?\"},\n",
    "    {'question': \"Which gas is most abundant in Earth's atmosphere?\"},\n",
    "    {'question': \"What color is a ripe banana?\"}\n",
    "]\n",
    "res = llm_chain.generate(qa)\n",
    "\n",
    "for question, answer in zip(qa, res.generations):\n",
    "    print(f'Question: {question[\"question\"]}\\nAnswer: {answer[0].text}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Including multiple questions in the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Paris\\nBlue Whale\\nNitrogen\\nYellow'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_template = \"\"\"Answer the following questions one at a time.\n",
    "\n",
    "Questions:\n",
    "{questions}\n",
    "\n",
    "Answers:\n",
    "\"\"\"\n",
    "\n",
    "long_prompt = PromptTemplate(template=multi_template, input_variables=['questions'])\n",
    "\n",
    "llm_chain = LLMChain(prompt=long_prompt, llm=llm)\n",
    "\n",
    "qs_str = (\n",
    "    \"What is the capital city of France?\\n\" +\n",
    "    \"What is the largest mammal on Earth?\\n\" +\n",
    "    \"Which gas is most abundant in Earth's atmosphere?\\n\" +\n",
    "\t\"What color is a ripe banana?\\n\"\n",
    ")\n",
    "\n",
    "llm_chain.run(qs_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain offers various modules for building language model applications, allowing users to combine them for more complex applications or use them individually for simpler ones, with the basic building block being calling an LLM on input, as demonstrated in the example of creating a company name based on its product.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model_name = 'gpt-3.5-turbo', temperature=0)\n",
    "\n",
    "summarization_template = \"Summarize the following text to one sentence: {text}\"\n",
    "summarization_prompt = PromptTemplate(input_variables=['text'], template=summarization_template)\n",
    "summarization_chain = LLMChain(llm=llm, prompt=summarization_prompt)\n",
    "\n",
    "text = \"LangChain provides many modules that can be used to build language model applications. Modules can be combined to create more complex applications, or be used individually for simple applications. The most basic building block of LangChain is calling an LLM on some input. Let’s walk through a simple example of how to do this. For this purpose, let’s pretend we are building a service that generates a company name based on what the company makes.\"\n",
    "summarized_text = summarization_chain.predict(text=text)\n",
    "\n",
    "summarized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Votre texte ici'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation_template = \"Translate the following text from {source_language} to {target_language}: {text}\"\n",
    "translation_prompt = PromptTemplate(input_variables=['source_language', 'target_language', 'text'], template=translation_template)\n",
    "translation_chain = LLMChain(llm=llm, prompt=translation_prompt)\n",
    "\n",
    "source_language = \"English\"\n",
    "target_language = \"French\"\n",
    "text = \"Your text here\"\n",
    "translated_text = translation_chain.predict(source_language=source_language, target_language=target_language, text=text)\n",
    "\n",
    "translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
